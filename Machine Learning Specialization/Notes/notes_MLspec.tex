\documentclass[11 pt]{scrartcl}
\usepackage[header, margin, koma]{tyler}
%\usetikzlibrary{automata,arrows,positioning,calc}
\usepackage{csquotes}

\pagestyle{fancy}
\fancyhf{}
\fancyhead[l]{Coursera Machine Learning Specialization Notes}
\fancyhead[r]{Raayan Dhar}
\cfoot{\thepage}

\begin{document} 
\title{\Large Coursera Machine Learning Specialization}
\author{\large Raayan Dhar}
\date{\large\today}

\maketitle 

\begin{center}
\begin{displayquote}
    \emph{"(Machine Learning) The field of study that gives computers the ability to learn without explicitly being programmed."} \\ \begin{flushright} \emph{â€“ Athur Samuel, 1959}.  \end{flushright}
\end{displayquote}
\end{center}


These are course notes for Deeplearning.ai's Machine Learning Specialization, taught by Professor Andrew Ng. Credit for the template goes to Tyler Zhu.

\tableofcontents 

\newpage

\section{Supervised Machine Learning}

\subsection{Week 1}
Supervised machine learning refers to algorithms that learn $x \rightarrow y$, or input to output mappings.
\newline\newline The key characteristic of supervised learning is that you give your learning algorithm examples to learn from. This includes
right answers (correct label $y$ for given input $x$). It is by seeing correct pairs of input $x$ and desired output label $y$
that the learning algorithm eventually learns to take \textit{just} the input alone without the output label, and gives a reasonably
accurate prediction or guess of the output. An example of a supervised learning algorithm is \textit{\underline{regression}}, where the task is to predict
a number. A second example of a supervised learning algorithm is \textit{\underline{classification}}, where the task is to predict
categories. Classification is different from regression because it only has a small number of possible output, unlike regression, which has infinite.
Additionally, classification algorithms can take two or more inputs. Typically, the more complex the problem, the more
inputs you will need to use. 
\begin{enumerate}
    \item Supervised learning learns from data labeled with the "right answers."
    \item Unsupervised learning finds something interesting in unlabeled data. In Unsupervised learning, 
    data comes with inputs $x$, but not output labels $y$. The algorithm has to find \underline{structure} in 
    the data. For example, 
    \begin{enumerate}
        \item Clustering: Groups similar data points together
        \item Anomaly detection: Find unusual data points
        \item Dimensionality reduction: compress data using fewer numbers
    \end{enumerate}
\end{enumerate}
\textbf{Terminology \& Notation}
\begin{enumerate}
    \item \textbf{Definition} (informal) Training Set: Data used to train the model.
    \item $x$ = ``input" variable
    \item $y$ = ``output'' variable / ``target'' variable
    \item $m$ = number of training examples
    \item $(x, y)$ = single training example
    \item $(x^{(i)}, y^{(i)})$ = \textit{i}th training example (e.g., 1st, 2nd, 3rd, etc)
    \item $w$ = weight (parameter)
    \item $b$ = bias (parameter)
    \item $f$ = function / model, also represented by $f_{w,b} (x) = wx + b$
\end{enumerate}
We will define the \textbf{cost function} to be


\end{document}
